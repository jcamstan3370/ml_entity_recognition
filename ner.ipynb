{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2976,
     "status": "ok",
     "timestamp": 1586634285249,
     "user": {
      "displayName": "Jared Stanley",
      "photoUrl": "",
      "userId": "12624706890601326749"
     },
     "user_tz": 360
    },
    "id": "6x3lpLBpWo92",
    "outputId": "b64ec9f7-6dc4-498a-8937-d070a943be4b"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.gold import GoldParse\n",
    "import spacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SJ9WGwrjiJwr",
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "notebook_out = None\n",
    "artefacts_temp_dir = None\n",
    "experiment_name = None\n",
    "mode = 'local'\n",
    "payload = {\n",
    "    'model': 'de_core_news_md' # 'de_core_news_md' 'en_core_web_sm'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24533,
     "status": "ok",
     "timestamp": 1586634313958,
     "user": {
      "displayName": "Jared Stanley",
      "photoUrl": "",
      "userId": "12624706890601326749"
     },
     "user_tz": 360
    },
    "id": "f02lqwM8Duv8",
    "outputId": "71b42306-8766-4d21-d036-d625d9d3e5b2"
   },
   "outputs": [],
   "source": [
    "# For Collab Runs\n",
    "if mode == 'google_collab': \n",
    "    import importlib\n",
    "    if importlib.util.find_spec('mlflow') is None:\n",
    "      !pip install mlflow\n",
    "    if importlib.util.find_spec('papermill') is None:\n",
    "      !pip install papermill\n",
    "    \n",
    "    # Run these commmands to get different language sets\n",
    "    # You must restart runtime to be able to do a subsequent load with spacy\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    !python -m spacy download de_core_news_md\n",
    "    \n",
    "    # TODO: Develop BERT \n",
    "    #!pip install spacy-transformers\n",
    "    #!python -m spacy download de_trf_bertbasecased_lg\n",
    "    #!pip install spacy-transformers\n",
    "    #!python -m spacy download en_trf_bertbasecased_lg\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    os.chdir('/content/gdrive')\n",
    "    !cd /content/gdrive\n",
    "    !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGbxacze-rv3"
   },
   "outputs": [],
   "source": [
    "# Set Experiment\n",
    "if (experiment_name): mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nUBt-3bWObga",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loads the Language Model\n",
    "model_types = [\n",
    "    'de_core_news_md',              # Medium German Core\n",
    "    'de_trf_bertbasecased_lg',      # German BERT\n",
    "    'en_core_web_sm',               # Small English Core\n",
    "    'en_core_web_md',               # Medium English Core\n",
    "    'en_core_web_lg',               # Medium English Core\n",
    "    'en_trf_bertbaseuncased_lg',    # English BERT\n",
    "    'fr_core_news_md',              # Medium French Core\n",
    "    'pt_core_news_sm',              # Small Portuguese Core\n",
    "]\n",
    "\n",
    "# Validate Choice\n",
    "if payload['model'] not in model_types: \n",
    "    raise(\"Invalid model type selected.\")\n",
    "\n",
    "# Loads selected language model\n",
    "# nlp = spacy.load(payload['model'])\n",
    "\n",
    "# Example Models\n",
    "#nlp_de = spacy.load('de_core_news_md')\n",
    "nlp_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_wrap(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print('-'*70)\n",
    "        func(*args, **kwargs)\n",
    "        print('-'*70)\n",
    "    return(wrapper)\n",
    "\n",
    "@pretty_wrap        \n",
    "def print_predict(nlp_model, sentences):\n",
    "    \n",
    "    docs = [nlp_model(s) for s in sentences]\n",
    "\n",
    "    for d in docs:\n",
    "        for ent in d.ents:\n",
    "            print({\n",
    "                'sentence': d,\n",
    "                'entity': ent.text,\n",
    "                'type': ent.label_,\n",
    "                'start_ind': ent.start_char,\n",
    "                'end_ind': ent.end_char\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1098,
     "status": "ok",
     "timestamp": 1576243767877,
     "user": {
      "displayName": "Pedro Sousa",
      "photoUrl": "",
      "userId": "16030587061696446315"
     },
     "user_tz": 0
    },
    "id": "dUk-c1-_P8gw",
    "outputId": "6fb67d08-1d3a-4e2c-8b9e-29a2d75e1feb"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'nlp_de' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2e2424f7a25f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m ]\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp_de\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp_de' is not defined"
     ]
    }
   ],
   "source": [
    "# NER German Examples\n",
    "\n",
    "sentences = [\n",
    "    \"Ich wohne in Berlin.\",\n",
    "    \"Ich studiere an der TUM.\",\n",
    "    \"Ich bin nicht Angela Merkel.\",\n",
    "    \"Mieten Sie ein 2019 smart EQ fortwo Coupé für nur 139 EUR / Monat.\"\n",
    "]\n",
    "\n",
    "print_predict(nlp_de, sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1049,
     "status": "ok",
     "timestamp": 1576243622292,
     "user": {
      "displayName": "Pedro Sousa",
      "photoUrl": "",
      "userId": "16030587061696446315"
     },
     "user_tz": 0
    },
    "id": "Fl3-SxYjHYb2",
    "outputId": "14f815da-e8f1-47be-e3a3-d22a27782efc"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "----------------------------------------------------------------------\n{'sentence': I am living in Colorado., 'entity': 'Colorado', 'type': 'GPE', 'start_ind': 15, 'end_ind': 23}\n{'sentence': In 2019, my rent was $1000 per month., 'entity': '2019', 'type': 'DATE', 'start_ind': 3, 'end_ind': 7}\n{'sentence': In 2019, my rent was $1000 per month., 'entity': '1000', 'type': 'MONEY', 'start_ind': 22, 'end_ind': 26}\n{'sentence': I really miss Obamba., 'entity': 'Obamba', 'type': 'GPE', 'start_ind': 14, 'end_ind': 20}\n{'sentence': This year, I will get a Tesla., 'entity': 'This year', 'type': 'DATE', 'start_ind': 0, 'end_ind': 9}\n{'sentence': This year, I will get a Tesla., 'entity': 'Tesla', 'type': 'WORK_OF_ART', 'start_ind': 24, 'end_ind': 29}\n----------------------------------------------------------------------\n"
    }
   ],
   "source": [
    "# NER English Examples\n",
    "\n",
    "sentences = [\n",
    "    \"I am living in Colorado.\",\n",
    "    \"In 2019, my rent was $1000 per month.\",\n",
    "    \"I really miss Obamba.\",\n",
    "    \"This year, I will get a Tesla.\"\n",
    "]\n",
    "\n",
    "print_predict(nlp_en, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1016,
     "status": "ok",
     "timestamp": 1576243858321,
     "user": {
      "displayName": "Pedro Sousa",
      "photoUrl": "",
      "userId": "16030587061696446315"
     },
     "user_tz": 0
    },
    "id": "BFQj0uQkY1ll",
    "outputId": "404c6d55-7a35-457c-ccb8-bc4a0a939729"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[(0, 4, 'custom_entity')]]\n"
    }
   ],
   "source": [
    "# Custom Entity Training (Option 1: Manual)\n",
    "\n",
    "training_data = [      \n",
    "    (\"Tesla is incredible!\", \n",
    "     {\"entities\": [(0, 4, \"custom_entity\")]}\n",
    "    )\n",
    "]\n",
    "\n",
    "print([i[1]['entities'] for i in training_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1032,
     "status": "ok",
     "timestamp": 1576243912159,
     "user": {
      "displayName": "Pedro Sousa",
      "photoUrl": "",
      "userId": "16030587061696446315"
     },
     "user_tz": 0
    },
    "id": "Tv8laOxRZEOT",
    "outputId": "4f0fde76-566f-4765-d451-eb1b00d95489"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[(0, 5, 'COMPANY')], [(23, 28, 'COMPANY')], [(21, 26, 'COMPANY')], [(12, 17, 'COMPANY')], [(0, 5, 'COMPANY'), (45, 50, 'COMPANY')], [(0, 5, 'COMPANY'), (45, 50, 'COMPANY')], [(45, 50, 'COMPANY')], [(41, 46, 'COMPANY')], [(0, 5, 'COMPANY')], [(36, 41, 'COMPANY'), (110, 115, 'COMPANY')], [(36, 41, 'COMPANY'), (110, 115, 'COMPANY')]]\n"
    }
   ],
   "source": [
    "# Custom Entity Training (Option 2: Parse)\n",
    "\n",
    "texts = [\n",
    "    \"Tesla does not currently have full self-driving, but is level 2 ready.\",\n",
    "    \"Investors are mixed on Tesla stock.\",\n",
    "    \"If you want to buy a Tesla, it will likely cost you more than $50,000.\",\n",
    "    \"People like Tesla because it is a environmentally friendly company.\",\n",
    "    \"Tesla really moved the needle on EV adoption.\"\n",
    "    \"Tesla?\",\n",
    "    \"The new Gigafactory in Shanghai was built by Tesla in about a year.\",\n",
    "    \"Elon Musk was very close to failing with Tesla.\",\n",
    "    \"Tesla is also heavily focused on battery and solar products.\",\n",
    "    \"The supercharger network created by Tesla is helping support EV adoption.\"\n",
    "    \"A new battery is in development from Tesla for over 1 million miles.\"   \n",
    "]\n",
    "\n",
    "# Run the Parser\n",
    "matcher = Matcher(nlp_en.vocab)\n",
    "pattern1 = [{\"LOWER\": \"tesla\"}]\n",
    "matcher.add(\"COMPANY\", None, pattern1)\n",
    "new_labels = ['COMPANY']\n",
    "training_data = []\n",
    "\n",
    "for text in texts:\n",
    "    doc = nlp_en(text)\n",
    "    matches = matcher(doc)\n",
    "    entities = []\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp_en.vocab.strings[match_id]  # Get string representation\n",
    "        span = doc[start:end]  # The matched span\n",
    "        #print(span.start_char, span.end_char)\n",
    "        entities.append((span.start_char, span.end_char, string_id))\n",
    "        #print(string_id, start, end, span.text)\n",
    "        training_data.append((text, {'entities': entities}))\n",
    "\n",
    "print([i[1]['entities'] for i in training_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vPTkzHclNhZe"
   },
   "outputs": [],
   "source": [
    "# Training Routine, for Training spaCy Models\n",
    "def train_model(\n",
    "        nlp_model, \n",
    "        training_data, \n",
    "        revision_data = None, \n",
    "        labels = None,\n",
    "        output_dir = None, \n",
    "        n_iter = 30,\n",
    "        dropout = 0.35\n",
    "    ):\n",
    "    \n",
    "    \"\"\" Set up the pipeline and entity recognizer, and train the new entity. \"\"\"\n",
    "\n",
    "    nlp = nlp_model\n",
    "    ner = nlp_model.get_pipe(\"ner\")\n",
    "\n",
    "    # Add new entity labels to NER model\n",
    "    if labels:\n",
    "        for i in labels:\n",
    "            ner.add_label(i)  \n",
    "\n",
    "    optimizer = nlp.resume_training()\n",
    "    move_names = list(ner.move_names)\n",
    "\n",
    "    # Get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        \n",
    "        \n",
    "        # Include Revision data if given\n",
    "        if revision_data is None:\n",
    "            examples = training_data\n",
    "        else:\n",
    "            print('Applying Revisions...')\n",
    "            examples = revision_data + training_data\n",
    "\n",
    "        # n_iter is number of epochs\n",
    "        for itn in range(n_iter): \n",
    "            \n",
    "            print(\"{} of {}\".format(itn + 1, n_iter))\n",
    "            # Shuffle new each time\n",
    "            random.shuffle(examples)\n",
    "\n",
    "            # Batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(examples, size=compounding(1.0, 4.0, 1.001))\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts, \n",
    "                    annotations, \n",
    "                    sgd=optimizer, \n",
    "                    drop=dropout, \n",
    "                    losses=losses\n",
    "                )\n",
    "                \n",
    "            #print(\"Losses\", losses)\n",
    "            \n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "        \n",
    "            \n",
    "    return (nlp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 27578,
     "status": "ok",
     "timestamp": 1576244347767,
     "user": {
      "displayName": "Pedro Sousa",
      "photoUrl": "",
      "userId": "16030587061696446315"
     },
     "user_tz": 0
    },
    "id": "BQDUUJFBZW43",
    "outputId": "b49a40c6-f2c9-4bd0-dac1-b4a956b647e1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1 of 30\n2 of 30\n3 of 30\n4 of 30\n5 of 30\n6 of 30\n7 of 30\n8 of 30\n9 of 30\n10 of 30\n11 of 30\n12 of 30\n13 of 30\n14 of 30\n15 of 30\n16 of 30\n17 of 30\n18 of 30\n19 of 30\n20 of 30\n21 of 30\n22 of 30\n23 of 30\n24 of 30\n25 of 30\n26 of 30\n27 of 30\n28 of 30\n29 of 30\n30 of 30\nSaved model to test_model\n"
    }
   ],
   "source": [
    "#from copy import deepcopy\n",
    "#nlp_train = deepcopy(nlp_en)\n",
    "\n",
    "# Train the Model\n",
    "nlp_model = train_model(\n",
    "    nlp_en, \n",
    "    training_data, \n",
    "    labels = ['COMPANY'],\n",
    "    output_dir = './test_model'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Can you tell that I love Tesla? 6 7 Tesla COMPANY\nCan you tell that I love Tesla? 6 7 Tesla COMPANY\n"
    }
   ],
   "source": [
    "# Load model from disk and test\n",
    "test = spacy.load(\"./test_model\")\n",
    "doc = test(\"Can you tell that I love Tesla?\")\n",
    "for ent in doc.ents:\n",
    "    print(doc, ent.start, ent.end, ent.text, ent.label_)\n",
    "    \n",
    "# Use one in memory\n",
    "doc = nlp_model(\"Can you tell that I love Tesla?\")\n",
    "for ent in doc.ents:\n",
    "    print(doc, ent.start, ent.end, ent.text, ent.label_)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QXmz7miZTfSA"
   },
   "outputs": [],
   "source": [
    "# Revision Data is needed when training with new labels!\n",
    "# See: Catestrophic forgetting resolution\n",
    "# https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting\n",
    "\n",
    "# Make a copy for revision\n",
    "nlp_en_revised = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add lots of texts here for training!\n",
    "revision_texts = [\n",
    "  \"This is a revision text.\"\n",
    "]\n",
    "\n",
    "revision_data = []\n",
    "for doc in nlp_en_revised.pipe(revision_texts):\n",
    "    tags = [w.tag_ for w in doc]\n",
    "    heads = [w.head.i for w in doc]\n",
    "    deps = [w.dep_ for w in doc]\n",
    "    entities = [(e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "    revision_data.append((doc, GoldParse(doc, tags=tags, heads=heads,\n",
    "                                         deps=deps, entities=entities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17174,
     "status": "ok",
     "timestamp": 1576244465192,
     "user": {
      "displayName": "Pedro Sousa",
      "photoUrl": "",
      "userId": "16030587061696446315"
     },
     "user_tz": 0
    },
    "id": "QCANAMdgvXTX",
    "outputId": "5934388c-f480-4213-e5cd-84af5598c264"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Applying Revisions...\n1 of 30\n2 of 30\n3 of 30\n4 of 30\n5 of 30\n6 of 30\n7 of 30\n8 of 30\n9 of 30\n10 of 30\n11 of 30\n12 of 30\n13 of 30\n14 of 30\n15 of 30\n16 of 30\n17 of 30\n18 of 30\n19 of 30\n20 of 30\n21 of 30\n22 of 30\n23 of 30\n24 of 30\n25 of 30\n26 of 30\n27 of 30\n28 of 30\n29 of 30\n30 of 30\n"
    }
   ],
   "source": [
    "nlp_model_2 = train_model(\n",
    "    nlp_en, \n",
    "    training_data,  \n",
    "    revision_data = revision_data, \n",
    "    labels = new_labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-> Tesla COMPANY\n"
    }
   ],
   "source": [
    "doc = nlp_model_2(\"How much does a Tesla cost?\")\n",
    "for ent in doc.ents:\n",
    "    print('->', ent.text, ent.label_)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "entity_spacy_full_capability_example.ipynb",
   "provenance": [
    {
     "file_id": "1UPAbyE0cEM7bHvKcm7jUHNbly8hgigWi",
     "timestamp": 1573886459178
    },
    {
     "file_id": "1hTLJhL-px7SmjZ5ckJ3ijh0pPpMwhn1t",
     "timestamp": 1573886263923
    },
    {
     "file_id": "12LmyAHnau4XdE0YuuVxqjrbb5sU3JpLA",
     "timestamp": 1573884663567
    }
   ]
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}