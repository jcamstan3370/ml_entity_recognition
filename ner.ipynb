{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2976,
     "status": "ok",
     "timestamp": 1586634285249,
     "user": {
      "displayName": "Jared Stanley",
      "photoUrl": "",
      "userId": "12624706890601326749"
     },
     "user_tz": 360
    },
    "id": "6x3lpLBpWo92",
    "outputId": "b64ec9f7-6dc4-498a-8937-d070a943be4b"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import pickle\n",
    "import plac\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.gold import GoldParse\n",
    "import spacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SJ9WGwrjiJwr",
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "notebook_out = None\n",
    "artefacts_temp_dir = None\n",
    "experiment_name = None\n",
    "mode = 'local'\n",
    "payload = {\n",
    "    'model': 'de_core_news_md' # 'de_core_news_md' 'en_core_web_sm'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24533,
     "status": "ok",
     "timestamp": 1586634313958,
     "user": {
      "displayName": "Jared Stanley",
      "photoUrl": "",
      "userId": "12624706890601326749"
     },
     "user_tz": 360
    },
    "id": "f02lqwM8Duv8",
    "outputId": "71b42306-8766-4d21-d036-d625d9d3e5b2"
   },
   "outputs": [],
   "source": [
    "# For Collab Runs\n",
    "if mode == 'google_collab': \n",
    "    import importlib\n",
    "    if importlib.util.find_spec('mlflow') is None:\n",
    "      !pip install mlflow\n",
    "    if importlib.util.find_spec('papermill') is None:\n",
    "      !pip install papermill\n",
    "    \n",
    "    # Run these commmands to get different language sets\n",
    "    # You must restart runtime to be able to do a subsequent load with spacy\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    !python -m spacy download de_core_news_md\n",
    "    \n",
    "    # TODO: Develop BERT \n",
    "    #!pip install spacy-transformers\n",
    "    #!python -m spacy download de_trf_bertbasecased_lg\n",
    "    #!pip install spacy-transformers\n",
    "    #!python -m spacy download en_trf_bertbasecased_lg\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    os.chdir('/content/gdrive')\n",
    "    !cd /content/gdrive\n",
    "    !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGbxacze-rv3"
   },
   "outputs": [],
   "source": [
    "# Set Experiment\n",
    "if (experiment_name): mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nUBt-3bWObga",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loads the Language Model\n",
    "model_types = [\n",
    "    'de_core_news_md',              # Medium German Core\n",
    "    'de_trf_bertbasecased_lg',      # German BERT\n",
    "    'en_core_web_sm',               # Small English Core\n",
    "    'en_core_web_md',               # Medium English Core\n",
    "    'en_core_web_lg',               # Medium English Core\n",
    "    'en_trf_bertbaseuncased_lg',    # English BERT\n",
    "    'fr_core_news_md',              # Medium French Core\n",
    "    'pt_core_news_sm',              # Small Portuguese Core\n",
    "]\n",
    "\n",
    "# Validate Choice\n",
    "if payload['model'] not in model_types: \n",
    "    raise(\"Invalid model type selected.\")\n",
    "\n",
    "# Loads selected language model\n",
    "# nlp = spacy.load(payload['model'])\n",
    "\n",
    "# Example Models\n",
    "#nlp_de = spacy.load('de_core_news_md')\n",
    "nlp_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_wrap(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print('-'*70)\n",
    "        func(*args, **kwargs)\n",
    "        print('-'*70)\n",
    "    return(wrapper)\n",
    "\n",
    "@pretty_wrap        \n",
    "def print_predict(nlp_model, sentences):\n",
    "    \n",
    "    docs = [nlp_model(s) for s in sentences]\n",
    "\n",
    "    for d in docs:\n",
    "        for ent in d.ents:\n",
    "            print({\n",
    "                'sentence': d,\n",
    "                'entity': ent.text,\n",
    "                'type': ent.label_,\n",
    "                'start_ind': ent.start_char,\n",
    "                'end_ind': ent.end_char\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1098,
     "status": "ok",
     "timestamp": 1576243767877,
     "user": {
      "displayName": "Pedro Sousa",
      "photoUrl": "",
      "userId": "16030587061696446315"
     },
     "user_tz": 0
    },
    "id": "dUk-c1-_P8gw",
    "outputId": "6fb67d08-1d3a-4e2c-8b9e-29a2d75e1feb"
   },
   "outputs": [],
   "source": [
    "# NER German Examples\n",
    "\n",
    "sentences = [\n",
    "    \"Ich wohne in Berlin.\",\n",
    "    \"Ich studiere an der TUM.\",\n",
    "    \"Ich bin nicht Angela Merkel.\",\n",
    "    \"Mieten Sie ein 2019 smart EQ fortwo Coupé für nur 139 EUR / Monat.\"\n",
    "]\n",
    "\n",
    "print_predict(nlp_de, sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1049,
     "status": "ok",
     "timestamp": 1576243622292,
     "user": {
      "displayName": "Pedro Sousa",
      "photoUrl": "",
      "userId": "16030587061696446315"
     },
     "user_tz": 0
    },
    "id": "Fl3-SxYjHYb2",
    "outputId": "14f815da-e8f1-47be-e3a3-d22a27782efc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "{'sentence': I am living in Colorado., 'entity': 'Colorado', 'type': 'GPE', 'start_ind': 15, 'end_ind': 23}\n",
      "{'sentence': In 2019, my rent was $1000 per month., 'entity': '2019', 'type': 'DATE', 'start_ind': 3, 'end_ind': 7}\n",
      "{'sentence': In 2019, my rent was $1000 per month., 'entity': '1000', 'type': 'MONEY', 'start_ind': 22, 'end_ind': 26}\n",
      "{'sentence': I really miss Obamba., 'entity': 'Obamba', 'type': 'GPE', 'start_ind': 14, 'end_ind': 20}\n",
      "{'sentence': This year, I will get a Tesla., 'entity': 'This year', 'type': 'DATE', 'start_ind': 0, 'end_ind': 9}\n",
      "{'sentence': This year, I will get a Tesla., 'entity': 'Tesla', 'type': 'WORK_OF_ART', 'start_ind': 24, 'end_ind': 29}\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# NER English Examples\n",
    "\n",
    "sentences = [\n",
    "    \"I am living in Colorado.\",\n",
    "    \"In 2019, my rent was $1000 per month.\",\n",
    "    \"I really miss Obamba.\",\n",
    "    \"This year, I will get a Tesla.\"\n",
    "]\n",
    "\n",
    "print_predict(nlp_en, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1016,
     "status": "ok",
     "timestamp": 1576243858321,
     "user": {
      "displayName": "Pedro Sousa",
      "photoUrl": "",
      "userId": "16030587061696446315"
     },
     "user_tz": 0
    },
    "id": "BFQj0uQkY1ll",
    "outputId": "404c6d55-7a35-457c-ccb8-bc4a0a939729"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(19, 27, 'custom_entity')]]\n"
     ]
    }
   ],
   "source": [
    "# Custom Entity Training (Option 1: Manual)\n",
    "\n",
    "training_data = [      \n",
    "    (\"Lease a 2019 smart EQ fortwo coupe for as little as $139/month\", \n",
    "     {\"entities\": [(19, 27, \"custom_entity\")]}\n",
    "    )\n",
    "]\n",
    "\n",
    "print([i[1]['entities'] for i in training_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1032,
     "status": "ok",
     "timestamp": 1576243912159,
     "user": {
      "displayName": "Pedro Sousa",
      "photoUrl": "",
      "userId": "16030587061696446315"
     },
     "user_tz": 0
    },
    "id": "Tv8laOxRZEOT",
    "outputId": "4f0fde76-566f-4765-d451-eb1b00d95489"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(22, 28, 'MODEL')], [(30, 36, 'MODEL')], [(71, 77, 'MODEL')], [(20, 26, 'MODEL')], [(24, 30, 'MODEL'), (62, 68, 'MODEL'), (84, 90, 'MODEL')], [(24, 30, 'MODEL'), (62, 68, 'MODEL'), (84, 90, 'MODEL')], [(24, 30, 'MODEL'), (62, 68, 'MODEL'), (84, 90, 'MODEL')], [(0, 6, 'MODEL')], [(26, 32, 'MODEL'), (55, 61, 'MODEL')], [(26, 32, 'MODEL'), (55, 61, 'MODEL')], [(43, 49, 'MODEL')], [(3, 9, 'MODEL')], [(0, 6, 'MODEL'), (34, 40, 'MODEL')], [(0, 6, 'MODEL'), (34, 40, 'MODEL')]]\n"
     ]
    }
   ],
   "source": [
    "# Custom Entity Training (Option 2: Parse)\n",
    "\n",
    "texts = [\n",
    "    \"Lease a 2019 smart EQ fortwo coupe.\",\n",
    "    \"You can lease a 2019 smart EQ fortwo cabrio for as little as $199/month.\",\n",
    "    \"For navigating your city, or escaping it altogether, the 2019 smart EQ fortwo features a high-tech interior.\",\n",
    "    \"I Love the smart EQ fortwo car, it's amazing.\",\n",
    "    \"Reviews of the smart EQ fortwo have been phenomenal! Smart EQ fortwo cars are a hit.\"\n",
    "    \"Fortwo?\",\n",
    "    \"Fortwo is for everyone.\",\n",
    "    \"I am thinking of buying a fortwo coupe.\"\n",
    "    \"How much does a fortwo coupe cost?\",\n",
    "    \"At signing, how much will be due for a new fortwo?\",\n",
    "    \"Is fortwo the best car from Smart?\",\n",
    "    \"Fortwo is a type of Smart car.\"\n",
    "    \"The fortwo model is sick!\"   \n",
    "]\n",
    "\n",
    "# Run the Parser\n",
    "matcher = Matcher(nlp_en.vocab)\n",
    "pattern1 = [{\"LOWER\": \"fortwo\"}]\n",
    "matcher.add(\"MODEL\", None, pattern1)\n",
    "new_labels = ['MODEL']\n",
    "training_data = []\n",
    "\n",
    "for text in texts:\n",
    "    doc = nlp_en(text)\n",
    "    matches = matcher(doc)\n",
    "    entities = []\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp_en.vocab.strings[match_id]  # Get string representation\n",
    "        span = doc[start:end]  # The matched span\n",
    "        #print(span.start_char, span.end_char)\n",
    "        entities.append((span.start_char, span.end_char, string_id))\n",
    "        #print(string_id, start, end, span.text)\n",
    "        training_data.append((text, {'entities': entities}))\n",
    "\n",
    "print([i[1]['entities'] for i in training_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vPTkzHclNhZe"
   },
   "outputs": [],
   "source": [
    "# Training Routine, for Training spaCy Models\n",
    "def train_model(\n",
    "        nlp_model, \n",
    "        training_data, \n",
    "        revision_data = None, \n",
    "        labels = None,\n",
    "        output_dir = None, \n",
    "        n_iter = 30,\n",
    "        dropout = 0.35\n",
    "    ):\n",
    "    \n",
    "    \"\"\" Set up the pipeline and entity recognizer, and train the new entity. \"\"\"\n",
    "\n",
    "    nlp = nlp_model\n",
    "    ner = nlp_model.get_pipe(\"ner\")\n",
    "\n",
    "    # Add new entity labels to NER model\n",
    "    if labels:\n",
    "        for i in labels:\n",
    "            ner.add_label(i)  \n",
    "\n",
    "    optimizer = nlp.resume_training()\n",
    "    move_names = list(ner.move_names)\n",
    "\n",
    "    # Get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        \n",
    "        \n",
    "        # Include Revision data if given\n",
    "        if revision_data is None:\n",
    "            examples = training_data\n",
    "        else:\n",
    "            print('Applying Revisions...')\n",
    "            examples = revision_data + training_data\n",
    "\n",
    "        # n_iter is number of epochs\n",
    "        for itn in range(n_iter): \n",
    "            \n",
    "            print(\"{} of {}\".format(itn + 1, n_iter))\n",
    "            # Shuffle new each time\n",
    "            random.shuffle(examples)\n",
    "\n",
    "            # Batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(examples, size=compounding(1.0, 4.0, 1.001))\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts, \n",
    "                    annotations, \n",
    "                    sgd=optimizer, \n",
    "                    drop=dropout, \n",
    "                    losses=losses\n",
    "                )\n",
    "                \n",
    "            #print(\"Losses\", losses)\n",
    "            \n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "        \n",
    "            \n",
    "    return (nlp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 27578,
     "status": "ok",
     "timestamp": 1576244347767,
     "user": {
      "displayName": "Pedro Sousa",
      "photoUrl": "",
      "userId": "16030587061696446315"
     },
     "user_tz": 0
    },
    "id": "BQDUUJFBZW43",
    "outputId": "b49a40c6-f2c9-4bd0-dac1-b4a956b647e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 30\n",
      "2 of 30\n",
      "3 of 30\n",
      "4 of 30\n",
      "5 of 30\n",
      "6 of 30\n",
      "7 of 30\n",
      "8 of 30\n",
      "9 of 30\n",
      "10 of 30\n",
      "11 of 30\n",
      "12 of 30\n",
      "13 of 30\n",
      "14 of 30\n",
      "15 of 30\n",
      "16 of 30\n",
      "17 of 30\n",
      "18 of 30\n",
      "19 of 30\n",
      "20 of 30\n",
      "21 of 30\n",
      "22 of 30\n",
      "23 of 30\n",
      "24 of 30\n",
      "25 of 30\n",
      "26 of 30\n",
      "27 of 30\n",
      "28 of 30\n",
      "29 of 30\n",
      "30 of 30\n",
      "Saved model to test_model\n"
     ]
    }
   ],
   "source": [
    "#from copy import deepcopy\n",
    "#nlp_train = deepcopy(nlp_en)\n",
    "\n",
    "# Train the Model\n",
    "nlp_model = train_model(\n",
    "    nlp_en, \n",
    "    training_data, \n",
    "    labels = ['MODEL'],\n",
    "    output_dir = './test_model'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I lease a fortwo coupe? 5 6 fortwo MODEL\n",
      "How do I lease a fortwo coupe? 5 6 fortwo MODEL\n"
     ]
    }
   ],
   "source": [
    "# Load model from disk and test\n",
    "test = spacy.load(\"./test_model\")\n",
    "doc = test(\"How do I lease a fortwo coupe?\")\n",
    "for ent in doc.ents:\n",
    "    print(doc, ent.start, ent.end, ent.text, ent.label_)\n",
    "    \n",
    "# Use one in memory\n",
    "doc = nlp_model(\"How do I lease a fortwo coupe?\")\n",
    "for ent in doc.ents:\n",
    "    print(doc, ent.start, ent.end, ent.text, ent.label_)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QXmz7miZTfSA"
   },
   "outputs": [],
   "source": [
    "# Revision Data is needed when training with new labels!\n",
    "# See: Catestrophic forgetting resolution\n",
    "# https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting\n",
    "\n",
    "# Make a copy for revision\n",
    "nlp_en_revised = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add lots of texts here for training!\n",
    "revision_texts = [\n",
    "  \"I am really smart.\"\n",
    "]\n",
    "\n",
    "revision_data = []\n",
    "for doc in nlp_en_revised.pipe(revision_texts):\n",
    "    tags = [w.tag_ for w in doc]\n",
    "    heads = [w.head.i for w in doc]\n",
    "    deps = [w.dep_ for w in doc]\n",
    "    entities = [(e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "    revision_data.append((doc, GoldParse(doc, tags=tags, heads=heads,\n",
    "                                         deps=deps, entities=entities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17174,
     "status": "ok",
     "timestamp": 1576244465192,
     "user": {
      "displayName": "Pedro Sousa",
      "photoUrl": "",
      "userId": "16030587061696446315"
     },
     "user_tz": 0
    },
    "id": "QCANAMdgvXTX",
    "outputId": "5934388c-f480-4213-e5cd-84af5598c264"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Revisions...\n",
      "1 of 30\n",
      "2 of 30\n",
      "3 of 30\n",
      "4 of 30\n",
      "5 of 30\n",
      "6 of 30\n",
      "7 of 30\n",
      "8 of 30\n",
      "9 of 30\n",
      "10 of 30\n",
      "11 of 30\n",
      "12 of 30\n",
      "13 of 30\n",
      "14 of 30\n",
      "15 of 30\n",
      "16 of 30\n",
      "17 of 30\n",
      "18 of 30\n",
      "19 of 30\n",
      "20 of 30\n",
      "21 of 30\n",
      "22 of 30\n",
      "23 of 30\n",
      "24 of 30\n",
      "25 of 30\n",
      "26 of 30\n",
      "27 of 30\n",
      "28 of 30\n",
      "29 of 30\n",
      "30 of 30\n"
     ]
    }
   ],
   "source": [
    "nlp_model_2 = train_model(\n",
    "    nlp_en, \n",
    "    training_data,  \n",
    "    revision_data = revision_data, \n",
    "    labels = new_labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> fortwo MODEL\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_model_2(\"How do I lease a fortwo coupe?\")\n",
    "for ent in doc.ents:\n",
    "    print('->', ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "entity_spacy_full_capability_example.ipynb",
   "provenance": [
    {
     "file_id": "1UPAbyE0cEM7bHvKcm7jUHNbly8hgigWi",
     "timestamp": 1573886459178
    },
    {
     "file_id": "1hTLJhL-px7SmjZ5ckJ3ijh0pPpMwhn1t",
     "timestamp": 1573886263923
    },
    {
     "file_id": "12LmyAHnau4XdE0YuuVxqjrbb5sU3JpLA",
     "timestamp": 1573884663567
    }
   ]
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
